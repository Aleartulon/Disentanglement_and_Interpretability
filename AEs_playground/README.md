# What latent space do different AutoEncoders find?
The idea is to use different type of autoencoders and getting insights on interpretability and disentanglement.
For AE and VAE, input should have dimensions [B,T,C,nx_1,nx_2,...], where B is batch size, T is lenght of time series (in this repo it should always be T=1), C is the dimension of the vector field (C=1 if scalar solution), nx_1 is length of first dimension of the field, nx_2 is length of second dimension of the field and so on.

# Proper Orthogonal Decomposition (POD)

# AutoEncoders (AE)

# Variational AutoEncoders (VAE)